{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6fae6cac",
   "metadata": {},
   "source": [
    "### Data Ingestion (Handel different document types)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "34504c40",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\OWN SKILLS\\DS & Advance AI with Deployment\\Generative AI\\Projects\\multi-doc-type-RAG-system\\rag_env\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "USER_AGENT environment variable not set, consider setting it to identify your requests.\n"
     ]
    }
   ],
   "source": [
    "# Import dfferent loaders\n",
    "from langchain_community.document_loaders import PyPDFLoader, Docx2txtLoader, TextLoader, WebBaseLoader, UnstructuredMarkdownLoader\n",
    "\n",
    "def load_document(document_path):\n",
    "    \"\"\"\n",
    "    Loads a document from a given path or URL using the appropriate Langchain loader.\n",
    "    Args:\n",
    "        document_path (str): The path to the document file or a URL.\n",
    "    Returns:\n",
    "        list: A list of loaded documents.\n",
    "    Raises:\n",
    "        ValueError: If the document type is unsupported or the path is invalid.\n",
    "    \"\"\"\n",
    "    print(f\"Attempting to load document from: {document_path}\")\n",
    "    try:\n",
    "        if document_path.startswith(('http://', 'https://')):\n",
    "            loader = WebBaseLoader(document_path)\n",
    "        elif document_path.endswith('.pdf'):\n",
    "            loader = PyPDFLoader(document_path)\n",
    "        elif document_path.endswith(('.docx', '.doc')):\n",
    "            loader = Docx2txtLoader(document_path)\n",
    "        elif document_path.endswith('.txt'):\n",
    "            loader = TextLoader(document_path)\n",
    "        elif document_path.endswith('.md'):\n",
    "            loader = UnstructuredMarkdownLoader(document_path)\n",
    "        else:\n",
    "            raise ValueError(f\"Unsupported document type: {document_path}. Please provide a PDF, DOCX, TXT file, .MD file or a URL.\")\n",
    "\n",
    "        document = loader.load()\n",
    "        print(f\"Successfully loaded {len(document)} pages/parts from {document_path}\")\n",
    "        return document\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading document {document_path}: {e}\")\n",
    "        raise ValueError(f\"Could not load document {document_path}. Error: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cfab510b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from urllib.parse import urlparse\n",
    "import os\n",
    "\n",
    "def extract_document_info(document, original_path):\n",
    "    \"\"\"\n",
    "    Extracts text, structure, and enhanced metadata from loaded Langchain document.\n",
    "    Adds 'doc_type' and ensures 'source', 'page', and 'section' are present.\n",
    "    Args:\n",
    "        document (list): A list of Langchain Document objects.\n",
    "        original_path (str): The original path or URL used to load the document.\n",
    "    Returns:\n",
    "        list: A list of dictionaries, each containing extracted info for a document part (page).\n",
    "    \"\"\"\n",
    "    all_extracted_data = [] # Initialize an empty list to store dictionaries for each document part\n",
    "\n",
    "    # Determine document type once based on the original_path\n",
    "    doc_type = \"unknown\"\n",
    "    if original_path.startswith(('http://', 'https://')):\n",
    "        doc_type = \"web\"\n",
    "    else:\n",
    "        _, ext = os.path.splitext(original_path)\n",
    "        if ext:\n",
    "            doc_type = ext.lstrip('.').lower()\n",
    "        if doc_type == 'doc': # Handle .doc being treated as docx\n",
    "            doc_type = 'docx'\n",
    "\n",
    "    for i, doc in enumerate(document):\n",
    "        # Extract core content for the current doc\n",
    "        text_content = doc.page_content\n",
    "        metadata = {}   # Initialize an empty metadata\n",
    "\n",
    "        # Create a new metadata dictionary for the current doc, initialized with its own metadata\n",
    "        current_doc_info = doc.metadata.copy()\n",
    "        # Add the determined doc_type to this metadata\n",
    "        metadata['doc_type'] = doc_type\n",
    "        # Ensure source, page, and section are present (or default)\n",
    "        # For 'source', prefer existing source from metadata, otherwise use original_path\n",
    "        metadata['source'] = current_doc_info.get('source', original_path)\n",
    "        # For 'page', prefer existing page from metadata, otherwise use index + 1\n",
    "        metadata['page'] = current_doc_info.get('page', i) + 1\n",
    "        # For 'section', use existing section from metadata, otherwise 'N/A'\n",
    "        metadata['section'] = current_doc_info.get('section', 'N/A')\n",
    "\n",
    "        # Create a dictionary for the current document part and append it to the list\n",
    "        all_extracted_data.append({\n",
    "            'text': text_content,\n",
    "            'metadata': metadata\n",
    "        })\n",
    "\n",
    "    return all_extracted_data # Return the list of all extracted data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5c04148",
   "metadata": {},
   "source": [
    "### Data Cleaning and Normalization (doc. type specific structured cleaning and generalized text normalizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fd079bae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function for text-normalization\n",
    "def normalize_text(text: str) -> str:\n",
    "    \"\"\"\n",
    "    General-purpose text normalization:\n",
    "    - unify line breaks\n",
    "    - collapse excessive spaces\n",
    "    - collapse many blank lines into single paragraph breaks\n",
    "    - trim leading/trailing whitespace\n",
    "\n",
    "    This should be applied AFTER structure-specific cleaning.\n",
    "    \"\"\"\n",
    "    # Normalize Windows-style newlines\n",
    "    text = text.replace(\"\\r\\n\", \"\\n\").replace(\"\\r\", \"\\n\")\n",
    "    # Collapse tabs into spaces\n",
    "    text = re.sub(r\"[ \\t]+\", \" \", text)\n",
    "    # Collapse 3+ blank lines into just 2 (paragraph separation)\n",
    "    text = re.sub(r\"\\n\\s*\\n\\s*\\n+\", \"\\n\\n\", text)\n",
    "    # Remove extra spaces around newlines\n",
    "    text = re.sub(r\"[ \\t]*\\n[ \\t]*\", \"\\n\", text)\n",
    "    # Finally, collapse multiple spaces again (in case we introduced any)\n",
    "    text = re.sub(r\" {2,}\", \" \", text)\n",
    "    # Strip leading/trailing whitespace\n",
    "    text = text.strip()\n",
    "\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "577ee143",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "def clean_document_structure(extracted_doc_list):\n",
    "    \"\"\"\n",
    "    Cleans the document text based on its document type.\n",
    "    Args:\n",
    "        extracted_doc_list (list): A list of dictionaries, each containing 'text' and 'metadata'.\n",
    "                                   'metadata' must contain 'doc_type'.\n",
    "    Returns:\n",
    "        list: An updated list of dictionaries with cleaned text for each document.\n",
    "    \"\"\"\n",
    "    cleaned_document_list = []\n",
    "\n",
    "    for extracted_doc_dict in extracted_doc_list:\n",
    "        if 'text' not in extracted_doc_dict or 'metadata' not in extracted_doc_dict:\n",
    "            raise ValueError(\"Each input dictionary must contain 'text' and 'metadata' keys.\")\n",
    "        if 'doc_type' not in extracted_doc_dict['metadata']:\n",
    "            raise ValueError(\"Metadata must contain 'doc_type' key.\")\n",
    "\n",
    "        raw_text = extracted_doc_dict['text']\n",
    "        doc_type = extracted_doc_dict['metadata']['doc_type']\n",
    "        cleaned_text = raw_text # Initialize with raw text, cleaning methods will modify this\n",
    "\n",
    "        print(f\"Cleaning document of type: {doc_type}\")\n",
    "        print(f\"Original text length: {len(raw_text)}\")\n",
    "\n",
    "        if doc_type == 'md':\n",
    "            print(\"Applying Markdown specific cleaning...\")\n",
    "            # Remove Markdown headers (e.g., # Header 1)\n",
    "            cleaned_text = re.sub(r'^#+\\s*(.*)$', r'\\1', cleaned_text, flags=re.MULTILINE)\n",
    "            # Remove bold and italic formatting\n",
    "            cleaned_text = re.sub(r'(\\S*?)(\\*{1,2}|_{1,2})(.*?)\\2', r'\\1\\3', cleaned_text) # Bold/Italic\n",
    "            # Remove links (display text only)\n",
    "            cleaned_text = re.sub(r'\\[(.*?)\\]\\(.*\\)', r'\\1', cleaned_text)\n",
    "            # Remove inline code blocks\n",
    "            cleaned_text = re.sub(r'`(.*?)`', r'\\1', cleaned_text)\n",
    "            # Remove blockquotes\n",
    "            cleaned_text = re.sub(r'^>\\s?', '', cleaned_text, flags=re.MULTILINE)\n",
    "            # Remove list markers\n",
    "            cleaned_text = re.sub(r'^[\\-*+]\\s?', '', cleaned_text, flags=re.MULTILINE)\n",
    "            # Remove emojis\n",
    "            cleaned_text = re.sub(r'[^\\x00-\\x7F]+', '', cleaned_text)\n",
    "        elif doc_type == 'web':\n",
    "            print(\"Applying Web specific cleaning with BeautifulSoup...\")\n",
    "            soup = BeautifulSoup(raw_text, 'html.parser')\n",
    "\n",
    "            # Remove script and style elements\n",
    "            for script_or_style in soup(['script', 'style']):\n",
    "                script_or_style.extract() # Remove them from the soup\n",
    "\n",
    "            # Get text\n",
    "            cleaned_text = soup.get_text()\n",
    "        else: # General text (pdf, docx, txt). No specific structural cleaning needed before normalization.\n",
    "            print(f\"No specific structural cleaning for {doc_type}. Applying general text normalization.\")\n",
    "            cleaned_text = raw_text\n",
    "\n",
    "        # Apply general text normalization as a final step for all document types\n",
    "        cleaned_text = normalize_text(cleaned_text)\n",
    "\n",
    "        # Update the text in the current dictionary and append to the new list\n",
    "        extracted_doc_dict['text'] = cleaned_text\n",
    "        cleaned_document_list.append(extracted_doc_dict)\n",
    "\n",
    "    return cleaned_document_list"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "306bd808",
   "metadata": {},
   "source": [
    "### Chunking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "33acc932",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Structure-aware splitting function 'structure_aware_splitter' defined.\n"
     ]
    }
   ],
   "source": [
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_core.documents import Document\n",
    "\n",
    "def structure_aware_splitter(extracted_doc_dict):\n",
    "    \"\"\"\n",
    "    Performs initial splitting based on natural document boundaries.\n",
    "    Args:\n",
    "        extracted_doc_dict (dict): A dictionary containing 'text' and 'metadata'.\n",
    "                                   'metadata' must contain 'doc_type', 'source', 'page', 'section'.\n",
    "    Returns:\n",
    "        list: A list of dictionaries, each representing a structurally aware chunk\n",
    "              with 'text' and updated 'metadata'.\n",
    "    \"\"\"\n",
    "    if 'text' not in extracted_doc_dict or 'metadata' not in extracted_doc_dict:\n",
    "        raise ValueError(\"Input dictionary must contain 'text' and 'metadata' keys.\")\n",
    "\n",
    "    raw_text = extracted_doc_dict['text']\n",
    "    metadata = extracted_doc_dict['metadata']\n",
    "    doc_type = metadata['doc_type']\n",
    "\n",
    "    print(f\"Applying structure-aware splitting for document type: {doc_type}\")\n",
    "\n",
    "    # Define separators based on document type\n",
    "    if doc_type == 'txt':\n",
    "        separators = ['\\n\\n', '\\n', ' ', ''] # Prioritize paragraphs for plain text\n",
    "    # Add more conditions for other document types if needed (e.g., 'md', 'web')\n",
    "    # For simplicity, using same separators for now but can be customized later.\n",
    "    else:\n",
    "        separators = ['\\n\\n', '\\n', ' ', ''] # Default separators\n",
    "\n",
    "    # Initialize RecursiveCharacterTextSplitter for initial structural chunks\n",
    "    # Larger chunk_size and no overlap for initial structural split\n",
    "    text_splitter = RecursiveCharacterTextSplitter(\n",
    "        separators=separators,\n",
    "        chunk_size=2000,  # Larger chunks for initial structural split\n",
    "        chunk_overlap=0,\n",
    "        length_function=len, # Character count for initial split\n",
    "        add_start_index=True\n",
    "    )\n",
    "\n",
    "    # Split the document's text\n",
    "    # The splitter expects a list of Document objects, so create one from the raw text.\n",
    "    doc_for_splitting = [Document(page_content=raw_text, metadata=metadata)]\n",
    "    split_documents = text_splitter.split_documents(doc_for_splitting)\n",
    "\n",
    "    # Format the split documents into the desired dictionary structure\n",
    "    formatted_chunks = []\n",
    "    for i, split_doc in enumerate(split_documents):\n",
    "        chunk_metadata = split_doc.metadata.copy()\n",
    "        # Update chunk metadata with more specific chunk information if needed\n",
    "        chunk_metadata['chunk_id'] = i\n",
    "        formatted_chunks.append({\n",
    "            'text': split_doc.page_content,\n",
    "            'metadata': chunk_metadata\n",
    "        })\n",
    "\n",
    "    print(f\"Original text split into {len(formatted_chunks)} structural chunks.\")\n",
    "    return formatted_chunks\n",
    "\n",
    "print(\"Structure-aware splitting function 'structure_aware_splitter' defined.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c420a90e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token counting function 'num_tokens_from_string' defined.\n"
     ]
    }
   ],
   "source": [
    "import tiktoken\n",
    "\n",
    "def num_tokens_from_string(text: str, model_name: str = \"cl100k_base\") -> int:\n",
    "    \"\"\"\n",
    "    Returns the number of tokens in a text string.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        encoding = tiktoken.encoding_for_model(model_name)\n",
    "    except KeyError:\n",
    "        # Fallback to a common encoding if model_name is not directly supported\n",
    "        encoding = tiktoken.get_encoding(\"cl100k_base\")\n",
    "    num_tokens = len(encoding.encode(text))\n",
    "    return num_tokens\n",
    "\n",
    "print(\"Token counting function 'num_tokens_from_string' defined.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c6f8c0b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length-based refinement function 'length_based_refinement' defined.\n"
     ]
    }
   ],
   "source": [
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_core.documents import Document\n",
    "\n",
    "def length_based_refinement(structural_chunks: list, target_chunk_size: int = 500, chunk_overlap: int = 100) -> list:\n",
    "    \"\"\"\n",
    "    Refines a list of structural chunks by further splitting any chunk exceeding a target\n",
    "    length using a RecursiveCharacterTextSplitter with token-based length function.\n",
    "\n",
    "    Args:\n",
    "        structural_chunks (list): A list of dictionaries, each containing 'text' and 'metadata',\n",
    "                                  output from structure_aware_splitter.\n",
    "        target_chunk_size (int): The desired maximum token length for refined chunks.\n",
    "        chunk_overlap (int): The number of tokens to overlap between sub-chunks.\n",
    "\n",
    "    Returns:\n",
    "        list: A list of dictionaries, each representing a refined chunk with 'text' and 'metadata'.\n",
    "    \"\"\"\n",
    "    refined_chunks = []\n",
    "\n",
    "    print(f\"Applying length-based refinement with target_chunk_size={target_chunk_size} and chunk_overlap={chunk_overlap}.\")\n",
    "\n",
    "    for i, structural_chunk in enumerate(structural_chunks):\n",
    "        text = structural_chunk['text']\n",
    "        metadata = structural_chunk['metadata'].copy()\n",
    "        current_chunk_tokens = num_tokens_from_string(text)\n",
    "\n",
    "        if current_chunk_tokens > target_chunk_size:\n",
    "            print(f\"  Chunk {i} (original tokens: {current_chunk_tokens}) exceeds target. Further splitting...\")\n",
    "            # Create a new splitter for this sub-splitting process\n",
    "            sub_splitter = RecursiveCharacterTextSplitter(\n",
    "                chunk_size=target_chunk_size,\n",
    "                chunk_overlap=chunk_overlap,\n",
    "                length_function=num_tokens_from_string, # Use token counting\n",
    "                add_start_index=True\n",
    "            )\n",
    "            # Convert the structural chunk into a Document object for the splitter\n",
    "            doc_to_split = Document(page_content=text, metadata=metadata)\n",
    "            sub_documents = sub_splitter.split_documents([doc_to_split])\n",
    "\n",
    "            for j, sub_doc in enumerate(sub_documents):\n",
    "                sub_chunk_metadata = sub_doc.metadata.copy()\n",
    "                # Update chunk_id to reflect it's a sub-chunk\n",
    "                sub_chunk_metadata['chunk_id'] = f\"{metadata.get('chunk_id', i)}-{j}\"\n",
    "                refined_chunks.append({\n",
    "                    'text': sub_doc.page_content,\n",
    "                    'metadata': sub_chunk_metadata\n",
    "                })\n",
    "        else:\n",
    "            print(f\"  Chunk {i} (tokens: {current_chunk_tokens}) is within target. Adding directly.\")\n",
    "            refined_chunks.append(structural_chunk)\n",
    "\n",
    "    print(f\"Total refined chunks after length-based refinement: {len(refined_chunks)}\")\n",
    "    return refined_chunks\n",
    "\n",
    "print(\"Length-based refinement function 'length_based_refinement' defined.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6955ef30",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chunking function 'chunk_document' defined.\n"
     ]
    }
   ],
   "source": [
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_core.documents import Document\n",
    "\n",
    "def chunk_document(cleaned_doc_list: list, target_chunk_size: int = 500, chunk_overlap: int = 100) -> list:\n",
    "    \"\"\"\n",
    "    Combines structure-aware splitting and length-based refinement into a single function\n",
    "    for processing cleaned documents into final chunks.\n",
    "\n",
    "    Args:\n",
    "        cleaned_doc_list (list): A list of dictionaries, each containing 'text' and 'metadata'\n",
    "                                   from the cleaned documents.\n",
    "        target_chunk_size (int): The desired maximum token length for refined chunks.\n",
    "        chunk_overlap (int): The number of tokens to overlap between sub-chunks.\n",
    "\n",
    "    Returns:\n",
    "        list: A list of dictionaries, each representing a final chunk with 'text' and 'metadata'.\n",
    "    \"\"\"\n",
    "    print(\"Starting document chunking process...\")\n",
    "    all_final_chunks = []\n",
    "\n",
    "    for i, extracted_doc_dict in enumerate(cleaned_doc_list):\n",
    "        print(f\"Processing document {i+1}/{len(cleaned_doc_list)} for chunking...\")\n",
    "        # Step 1: Perform structure-aware splitting\n",
    "        structural_chunks = structure_aware_splitter(extracted_doc_dict)\n",
    "\n",
    "        # Step 2: Perform length-based refinement\n",
    "        refined_chunks = length_based_refinement(\n",
    "            structural_chunks,\n",
    "            target_chunk_size=target_chunk_size,\n",
    "            chunk_overlap=chunk_overlap\n",
    "        )\n",
    "        all_final_chunks.extend(refined_chunks)\n",
    "\n",
    "    print(f\"Document chunking process completed. Generated {len(all_final_chunks)} total final chunks from all documents.\")\n",
    "    return all_final_chunks\n",
    "\n",
    "print(\"Chunking function 'chunk_document' defined.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ee3329c",
   "metadata": {},
   "source": [
    "### Embedding and Vector Store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "209dc38d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_community.embeddings import OllamaEmbeddings\n",
    "from langchain_core.documents import Document\n",
    "\n",
    "\n",
    "def create_vector_store(documents: list, embedder)-> FAISS:\n",
    "        \"\"\"This function create a FAISS vector store and return it.\n",
    "        Args:\n",
    "            documents (list): an list of chunk documents (dictionaries with 'text' and 'metadata')\n",
    "\n",
    "        Raises:\n",
    "            Exception: return an exception when, fails to initialise the vector store\n",
    "\n",
    "        Returns:\n",
    "            FAISS: return an vector store of FAISS\n",
    "        \"\"\"\n",
    "        try:\n",
    "            # Convert list of dictionaries to list of Document objects\n",
    "            langchain_documents = []\n",
    "            for i, doc in enumerate(documents):\n",
    "                if not isinstance(doc, dict):\n",
    "                    raise TypeError(f\"Expected a dictionary for document item {i}, but got {type(doc)}. Item: {doc}\")\n",
    "                if 'text' not in doc or 'metadata' not in doc:\n",
    "                    raise ValueError(f\"Document item {i} is missing 'text' or 'metadata' key. Item: {doc}\")\n",
    "                langchain_documents.append(Document(page_content=doc['text'], metadata=doc['metadata']))\n",
    "\n",
    "            vector_store = FAISS.from_documents(langchain_documents, embedder)\n",
    "            return vector_store\n",
    "        except Exception as e:\n",
    "          raise e"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9bd9fd5",
   "metadata": {},
   "source": [
    "### Chunk Retrieval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f0719d0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.cross_encoders import HuggingFaceCrossEncoder\n",
    "from langchain_classic.retrievers.document_compressors import CrossEncoderReranker\n",
    "from langchain_classic.retrievers import ContextualCompressionRetriever\n",
    "\n",
    "K_SEARCH = 50        # Initial vector search depth (k1)\n",
    "K_RERANK = 20        # Chunks to keep after re-ranking (k2)\n",
    "K_FINAL_CONTEXT = 10 # Final context chunks (k3)\n",
    "\n",
    "# 1. Load model\n",
    "cross_encoder = HuggingFaceCrossEncoder(\n",
    "    model_name=\"cross-encoder/ms-marco-MiniLM-L-6-v2\"\n",
    ")\n",
    "# 2. Create reranker\n",
    "reranker = CrossEncoderReranker(\n",
    "    model=cross_encoder,\n",
    "    top_n=K_RERANK\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7d635bda",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# def _initial_vector_search(query: str, vectorstore: FAISS) -> list[Document]:\n",
    "#         \"\"\"\n",
    "#         Step 1: Performs the initial, wide vector similarity search (k1).\n",
    "#         Handles Edge Case 1 (No candidates found).\n",
    "#         \"\"\"\n",
    "#         # --- 1. Initial Vector Search (Fetches K_SEARCH candidates) ---\n",
    "#         initial_candidates: list[Document] = vectorstore.similarity_search(\n",
    "#             query=query, \n",
    "#             k=K_SEARCH\n",
    "#         )\n",
    "\n",
    "#         # --- EDGE CASE CHECK 1: No candidates found ---\n",
    "#         if not initial_candidates:\n",
    "#             print(\"❌ WARNING: No chunks found in the vector store for the query.\")\n",
    "#             return []\n",
    "            \n",
    "#         total_chunks = len(initial_candidates)\n",
    "#         if total_chunks < K_SEARCH:\n",
    "#             print(f\"⚠️ INFO: Found only {total_chunks} chunks (less than k={K_SEARCH}). Proceeding with all available chunks.\")\n",
    "        \n",
    "#         return initial_candidates\n",
    "\n",
    "# def _apply_rerank(self, query: str, vectorstore: FAISS) -> list[Document]:\n",
    "#         \"\"\"\n",
    "#         Step 2: Applies the Cross-Encoder re-ranking to the initial candidates.\n",
    "#         Returns the top K_RERANK documents.\n",
    "#         \"\"\"\n",
    "#         # We use ContextualCompressionRetriever to apply the reranker.\n",
    "#         # It internally uses the vectorstore to fetch K_SEARCH documents first.\n",
    "#         compressor_retriever = ContextualCompressionRetriever(\n",
    "#             # Using vectorstore.as_retriever ensures the initial search k is respected\n",
    "#             base_retriever=self.vectorstore.as_retriever(search_kwargs={\"k\": K_SEARCH}),\n",
    "#             base_compressor=self.reranker,\n",
    "#         )\n",
    "        \n",
    "#         # Get the top K_RERANK chunks based on the cross-encoder score\n",
    "#         reranked_chunks: list[Document] = compressor_retriever.get_relevant_documents(query)\n",
    "        \n",
    "#         return reranked_chunks\n",
    "\n",
    "# def get_final_context_chunks(query: str, vectorstore: FAISS) -> list[Document]:\n",
    "#         \"\"\"\n",
    "#         Orchestrates the three-step retrieval process: Search -> Rerank -> MMR.\n",
    "#         \"\"\"\n",
    "        \n",
    "#         # 1. Initial Vector Search (handled implicitly by reranker's base retriever)\n",
    "#         # We call the search explicitly here for the sake of the initial logging/edge case check\n",
    "#         initial_chunks = _initial_vector_search(query, vectorstore)\n",
    "#         if not initial_chunks:\n",
    "#             return []\n",
    "            \n",
    "#         # 2. Re-ranking\n",
    "#         reranked_chunks = _apply_rerank(query, vectorstore)\n",
    "        \n",
    "#         # --- EDGE CASE CHECK 2: Insufficient chunks after reranking ---\n",
    "#         if not reranked_chunks or len(reranked_chunks) <= K_FINAL_CONTEXT:\n",
    "#              print(f\"⚠️ INFO: Only {len(reranked_chunks)} chunks left after re-ranking. Skipping MMR and returning all of them.\")\n",
    "#              # The list slice ensures we return at most K_FINAL_CONTEXT\n",
    "#              return reranked_chunks[K_FINAL_CONTEXT]\n",
    "\n",
    "#         # 3. Diversification (MMR)\n",
    "#         final_context_chunks = vectorstore.max_marginal_relevance_search(query)\n",
    "        \n",
    "#         print(f\"✅ Success: Retrieved {len(final_context_chunks)} final diverse context chunks.\")\n",
    "#         return final_context_chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20c4abf3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RetrievalSystem class defined with _initial_vector_search, _apply_rerank, and get_final_context_chunks methods.\n"
     ]
    }
   ],
   "source": [
    "# from langchain_community.cross_encoders import HuggingFaceCrossEncoder\n",
    "# from langchain_classic.retrievers.document_compressors import CrossEncoderReranker\n",
    "# from langchain_classic.retrievers import ContextualCompressionRetriever\n",
    "# from langchain_core.documents import Document\n",
    "# from langchain_community.vectorstores import FAISS # Assuming FAISS is the vectorstore type\n",
    "\n",
    "# K_SEARCH = 50        # Initial vector search depth (k1)\n",
    "# K_RERANK = 20        # Chunks to keep after re-ranking (k2)\n",
    "# K_FINAL_CONTEXT = 10 # Final context chunks (k3)\n",
    "\n",
    "# # 1. Load model\n",
    "# cross_encoder = HuggingFaceCrossEncoder(\n",
    "#     model_name=\"cross-encoder/ms-marco-MiniLM-L-6-v2\"\n",
    "# )\n",
    "# # 2. Create reranker instance\n",
    "# reranker_instance = CrossEncoderReranker(\n",
    "#     model=cross_encoder,\n",
    "#     top_n=K_RERANK\n",
    "# )\n",
    "\n",
    "# class RetrievalSystem:\n",
    "#     def __init__(self, vectorstore: FAISS, reranker: CrossEncoderReranker):\n",
    "#         self.vectorstore = vectorstore\n",
    "#         self.reranker = reranker\n",
    "\n",
    "#     def _initial_vector_search(self, query: str) -> list[Document]:\n",
    "#         \"\"\"\n",
    "#         Step 1: Performs the initial, wide vector similarity search (k1).\n",
    "#         Handles Edge Case 1 (No candidates found).\n",
    "#         \"\"\"\n",
    "#         # --- 1. Initial Vector Search (Fetches K_SEARCH candidates) ---\n",
    "#         initial_candidates: list[Document] = self.vectorstore.similarity_search(\n",
    "#             query=query,\n",
    "#             k=K_SEARCH\n",
    "#         )\n",
    "\n",
    "#         # --- EDGE CASE CHECK 1: No candidates found ---\n",
    "#         if not initial_candidates:\n",
    "#             print(\"❌ WARNING: No chunks found in the vector store for the query.\")\n",
    "#             return []\n",
    "\n",
    "#         total_chunks = len(initial_candidates)\n",
    "#         if total_chunks < K_SEARCH:\n",
    "#             print(f\"⚠️ INFO: Found only {total_chunks} chunks (less than k={K_SEARCH}). Proceeding with all available chunks.\")\n",
    "\n",
    "#         return initial_candidates\n",
    "\n",
    "#     def _apply_rerank(self, query: str, initial_candidates: list[Document]) -> list[Document]:\n",
    "#         \"\"\"\n",
    "#         Step 2: Applies the Cross-Encoder re-ranking to the initial candidates.\n",
    "#         Uses the reranker's compress_documents method directly on the candidates.\n",
    "#         Returns the top K_RERANK documents.\n",
    "#         \"\"\"\n",
    "#         print(f\"Applying reranking to {len(initial_candidates)} initial candidates...\")\n",
    "#         # The CrossEncoderReranker (a DocumentCompressor) has a compress_documents method\n",
    "#         # that takes a list of documents and a query, and returns the top_n reranked documents.\n",
    "#         reranked_chunks: list[Document] = self.reranker.compress_documents(\n",
    "#             documents=initial_candidates,\n",
    "#             query=query\n",
    "#         )\n",
    "#         print(f\"Reranking completed. Obtained {len(reranked_chunks)} chunks after reranking.\")\n",
    "#         return reranked_chunks\n",
    "\n",
    "#     def get_final_context_chunks(self, query: str) -> list[Document]:\n",
    "#         \"\"\"\n",
    "#         Orchestrates the three-step retrieval process: Search -> Rerank -> MMR.\n",
    "#         \"\"\"\n",
    "#         print(f\"Starting retrieval process for query: '{query}'\")\n",
    "\n",
    "#         # 1. Initial Vector Search (k1 = K_SEARCH)\n",
    "#         initial_candidates = self._initial_vector_search(query)\n",
    "#         if not initial_candidates:\n",
    "#             print(\"❌ Retrieval failed at initial vector search stage.\")\n",
    "#             return []\n",
    "#         print(f\"Initial vector search returned {len(initial_candidates)} candidates.\")\n",
    "\n",
    "#         # 2. Re-ranking (reduces to k2 = K_RERANK)\n",
    "#         reranked_chunks = self._apply_rerank(query, initial_candidates)\n",
    "\n",
    "#         # --- EDGE CASE CHECK 2: Insufficient chunks after reranking ---\n",
    "#         # If reranking yields fewer chunks than K_FINAL_CONTEXT, we just return what we have.\n",
    "#         if not reranked_chunks or len(reranked_chunks) <= K_FINAL_CONTEXT:\n",
    "#              print(f\"⚠️ INFO: Only {len(reranked_chunks)} chunks left after re-ranking (less than K_FINAL_CONTEXT={K_FINAL_CONTEXT}). Skipping MMR and returning all available reranked chunks.\")\n",
    "#              # Return all available reranked chunks, ensuring we don't return more than K_FINAL_CONTEXT if that's the upper limit\n",
    "#              return reranked_chunks[:K_FINAL_CONTEXT]\n",
    "\n",
    "#         # 3. Diversification (MMR) (reduces to k3 = K_FINAL_CONTEXT)\n",
    "#         # We perform MMR on the vector store. 'fetch_k' specifies the number of top\n",
    "#         # similarity results to consider for diversification (we use K_RERANK as a good pool size).\n",
    "#         # 'k' specifies the final number of diverse documents to return.\n",
    "#         print(f\"Applying MMR to diversify the top {K_RERANK} candidates to {K_FINAL_CONTEXT} final chunks...\")\n",
    "#         final_context_chunks = self.vectorstore.max_marginal_relevance_search(\n",
    "#             query=query,\n",
    "#             k=K_FINAL_CONTEXT,  # Number of diverse documents to return\n",
    "#             fetch_k=K_RERANK    # Number of documents to fetch for initial consideration for diversity\n",
    "#         )\n",
    "\n",
    "#         print(f\"✅ Success: Retrieved {len(final_context_chunks)} final diverse context chunks.\")\n",
    "#         return final_context_chunks\n",
    "\n",
    "# print(\"RetrievalSystem class defined with _initial_vector_search, _apply_rerank, and get_final_context_chunks methods.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b2226c0",
   "metadata": {},
   "source": [
    "### Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "299fd819",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attempting to load document from: data/PRITHA_MITRA.pdf\n",
      "Successfully loaded 3 pages/parts from data/PRITHA_MITRA.pdf\n",
      "Total page count before cleaning and normalization: 8456\n"
     ]
    }
   ],
   "source": [
    "doc_path = \"data/PRITHA_MITRA.pdf\"\n",
    "# Load document\n",
    "document = load_document(doc_path)\n",
    "# Extract info from doc\n",
    "extracted_doc_info = extract_document_info(document, doc_path)\n",
    "text_count = 0\n",
    "for doc in extracted_doc_info:\n",
    "    text_count += len(doc[\"text\"])\n",
    "\n",
    "print(f\"Total page count before cleaning and normalization: {text_count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "82f3a9df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaning document of type: pdf\n",
      "Original text length: 3184\n",
      "No specific structural cleaning for pdf. Applying general text normalization.\n",
      "Cleaning document of type: pdf\n",
      "Original text length: 2882\n",
      "No specific structural cleaning for pdf. Applying general text normalization.\n",
      "Cleaning document of type: pdf\n",
      "Original text length: 2390\n",
      "No specific structural cleaning for pdf. Applying general text normalization.\n",
      "Total page count after cleaning and normalization: 8456\n"
     ]
    }
   ],
   "source": [
    "# Clean the extracted document\n",
    "cleaned_doc_list = clean_document_structure(extracted_doc_info)\n",
    "text_count = 0\n",
    "for doc in cleaned_doc_list:\n",
    "    text_count += len(doc[\"text\"])\n",
    "\n",
    "print(f\"Total page count after cleaning and normalization: {text_count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "fe79c256",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting document chunking process...\n",
      "Processing document 1/3 for chunking...\n",
      "Applying structure-aware splitting for document type: pdf\n",
      "Original text split into 2 structural chunks.\n",
      "Applying length-based refinement with target_chunk_size=500 and chunk_overlap=100.\n",
      "  Chunk 0 (tokens: 429) is within target. Adding directly.\n",
      "  Chunk 1 (tokens: 232) is within target. Adding directly.\n",
      "Total refined chunks after length-based refinement: 2\n",
      "Processing document 2/3 for chunking...\n",
      "Applying structure-aware splitting for document type: pdf\n",
      "Original text split into 2 structural chunks.\n",
      "Applying length-based refinement with target_chunk_size=500 and chunk_overlap=100.\n",
      "  Chunk 0 (tokens: 394) is within target. Adding directly.\n",
      "  Chunk 1 (tokens: 215) is within target. Adding directly.\n",
      "Total refined chunks after length-based refinement: 2\n",
      "Processing document 3/3 for chunking...\n",
      "Applying structure-aware splitting for document type: pdf\n",
      "Original text split into 2 structural chunks.\n",
      "Applying length-based refinement with target_chunk_size=500 and chunk_overlap=100.\n",
      "  Chunk 0 (tokens: 441) is within target. Adding directly.\n",
      "  Chunk 1 (tokens: 84) is within target. Adding directly.\n",
      "Total refined chunks after length-based refinement: 2\n",
      "Document chunking process completed. Generated 6 total final chunks from all documents.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'text': 'Pritha Mitra\\n/linkedinhttps://www.linkedin.com/in/pritha-mitra-7219b6246/ |\\n/envel⌢peprithamitra208@gmail.com | ♂¶obile+91-8420274334\\nSummary\\nResults-driven Data Scientist with 2+ years of experience delivering AI/ML solutions in banking, finance,\\nand automation domains. Skilled in Python, R, SQL, PySpark, Oracle DB, NLP, OCR, Generative AI,\\nand API development, with a proven track record of 95% success in automation projects. Designed\\nand deployed LLM-powered document processing pipelines, financial forecasting systems, and end-to-end\\nML/DL applications that improved efficiency by up to 90%. Strong background in time-series forecasting,\\nmodel deployment, and data-driven decision making. Interested in translating complex problems into\\nscalable, real-world solutions that drive measurable business impact.\\nWork Experience\\nAI-ML Innovation Engineer, SimplyFi, Mumbai Oct 2024 – Present\\n1. AI-Powered Document Processing & Classification System:\\n• Designed and deployed a secure, end-to-end document processing pipeline using Mongo DB,\\nPaddleOCR, Tesseract, and LLMs, improving processing speed and accuracy by 90%.\\n• Integrated with Newgen API to ingest documents and classify them into four categories using\\ntemplate matching and sentiment-based keyword extraction, using word-level classi-\\nfication to handle edge cases and improve classification robustness.\\n• Built an invoice processing module leveraging Ollama LLMs, achieving over 80% accu-\\nracy in key data extraction.\\n• Implemented coordinate-based OCR highlighting and image snippet generation using\\nbox num, word num, and rec num metadata for traceability.\\n• Automated HS code retrievalusing Selenium-based web scraping from DGFT and integrated\\ndocument/PDF handling via APIs and Excel.\\n• Developed a fuzzy matching consistency check module to cross-verify data across Newgen,\\nCRL, and invoice sources, ensuring high data reliability.\\n• Integrated also pdf plumber to directly readle option for pdf without converting into image .',\n",
       " 'metadata': {'doc_type': 'pdf',\n",
       "  'source': 'data/PRITHA_MITRA.pdf',\n",
       "  'page': 1,\n",
       "  'section': 'N/A',\n",
       "  'start_index': 0,\n",
       "  'chunk_id': 0}}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Now check the chunking\n",
    "chunks = chunk_document(cleaned_doc_list)\n",
    "chunks[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "fa3f4f09",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[ 2025-12-11 13:20:39,165 ] root - INFO - Initializing the Ollama embedder.\n",
      "[ 2025-12-11 13:20:39,693 ] root - INFO - Initializing the Ollama embedder.\n",
      "[ 2025-12-11 13:20:44,540 ] httpx - INFO - HTTP Request: POST http://127.0.0.1:11434/api/embed \"HTTP/1.1 200 OK\"\n",
      "[ 2025-12-11 13:20:44,545 ] faiss.loader - INFO - Loading faiss with AVX2 support.\n",
      "[ 2025-12-11 13:20:44,567 ] faiss.loader - INFO - Successfully loaded faiss with AVX2 support.\n"
     ]
    }
   ],
   "source": [
    "# Create vector store\n",
    "from langchain_ollama.embeddings import OllamaEmbeddings\n",
    "from src.embedding.embedder import OllamaEmbedder\n",
    "from src.vectorstore.faiss_store import FaissVectorStore\n",
    "embedder = OllamaEmbedder().get_embedder()\n",
    "# vector_store = create_vector_store(final_chunks, embedder)\n",
    "vector_store = FaissVectorStore().create_vector_store(chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e06e22f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[ 2025-12-11 13:20:47,465 ] sentence_transformers.cross_encoder.CrossEncoder - INFO - Use pytorch device: cpu\n"
     ]
    }
   ],
   "source": [
    "# 1. Load model\n",
    "cross_encoder = HuggingFaceCrossEncoder(\n",
    "    model_name=\"cross-encoder/ms-marco-MiniLM-L-6-v2\"\n",
    ")\n",
    "# 2. Create reranker instance\n",
    "reranker_instance = CrossEncoderReranker(\n",
    "    model=cross_encoder,\n",
    "    top_n=K_RERANK\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rag_env (3.13.7)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
